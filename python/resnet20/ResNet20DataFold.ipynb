{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import os\n",
    "import csv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import pandas as pd\n",
    "from resnet20 import ResNet20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fold_batch_norm(model: nn.Module, data_path):\n",
    "    \"\"\"\n",
    "    Fold BatchNorm layers into Conv layers, then save the weights and biases\n",
    "    of each layer into separate CSV files in the specified `data_path`.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)  # Create the directory if it doesn't exist\n",
    "\n",
    "    conv_module = None\n",
    "    bn_next = False\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for index, (name, module) in enumerate(model.named_modules()):\n",
    "        # Identify Conv2d layers and handle BatchNorm folding\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            bn_next = True\n",
    "            conv_module = module\n",
    "        if isinstance(module, nn.BatchNorm2d) and bn_next:\n",
    "            bn_next = False\n",
    "\n",
    "            gamma = module.weight.data  # The scale factor\n",
    "            beta = module.bias.data  # The shift factor\n",
    "            mean = module.running_mean  # The moving average of means\n",
    "            var = module.running_var  # The moving average of variances\n",
    "            epsilon = module.eps  # Epsilon to avoid division by zero\n",
    "\n",
    "            # Compute the new Conv2d weights (fold BatchNorm into Conv)\n",
    "            with torch.no_grad():\n",
    "                # Scale Conv weights by gamma / sqrt(var + epsilon)\n",
    "                conv_module.weight.data = conv_module.weight.data * (gamma / torch.sqrt(var + epsilon)).view(-1, 1, 1, 1)\n",
    "\n",
    "                if \".bn\" in name:\n",
    "                    name_to_save = name.replace(\".bn\", \"_conv\")\n",
    "                elif \".shortcut_bn\" in name:  # Handling shortcut BatchNorm\n",
    "                    name_to_save = name.replace(\".shortcut_bn\", \"_shortcut\")\n",
    "                     # If there is a stride (downsampling), scale the weights and biases accordingly\n",
    "                    # if conv_module.stride != (1, 1):  # Check if stride > 1 (downsampling)\n",
    "                        # When stride > 1, we need to scale the gamma and beta by the stride factor\n",
    "                        # scale_factor = torch.sqrt(torch.tensor(1.0 / (conv_module.stride[0] ** 2)))\n",
    "                        # conv_module.weight.data *= scale_factor.view(-1, 1, 1, 1)\n",
    "                        # gamma *= scale_factor\n",
    "                        # beta *= scale_factor\n",
    "                    # print(f\"Processing shortcut BN: {name_to_save}\")\n",
    "                elif \"bn1\" in name:\n",
    "                    name_to_save = name.replace(\"bn1\", \"layer0_conv1\")\n",
    "\n",
    "                # Save weights\n",
    "                weights_filename = os.path.join(data_path, f'{name_to_save}_weight.csv')\n",
    "                print(f'weight name: {weights_filename}')\n",
    "                clean_data = conv_module.weight.data.numpy() if hasattr(conv_module.weight.data, 'numpy') else conv_module.weight.data\n",
    "                with open(weights_filename, mode='w', newline='') as file:\n",
    "                    writer = csv.writer(file)\n",
    "                    writer.writerows(clean_data.flatten().reshape(1, -1))\n",
    "\n",
    "                # Adjust Conv bias using BatchNorm parameters\n",
    "                if conv_module.bias is not None:\n",
    "                    conv_module.bias.data = (gamma * (conv_module.bias.data - mean) / torch.sqrt(var + epsilon)) + beta\n",
    "                    \n",
    "                    # Save bias\n",
    "                    biases_filename = os.path.join(data_path, f'{name_to_save}_bias.csv')\n",
    "                    print(f'bias name: {biases_filename}')\n",
    "                    clean_bias_data = conv_module.bias.numpy() if hasattr(conv_module.bias, 'numpy') else conv_module.bias\n",
    "                    with open(biases_filename, mode='w', newline='') as file:\n",
    "                        writer = csv.writer(file)\n",
    "                        writer.writerows(clean_bias_data.flatten().reshape(1, -1))\n",
    "                else:\n",
    "                    # If Conv2d doesn't have a bias, add one using BatchNorm parameters\n",
    "                    biases_filename = os.path.join(data_path, f'{name_to_save}_bias.csv')\n",
    "                    print(f'bias name: {biases_filename}')\n",
    "                    conv_module.bias = nn.Parameter((gamma * (-mean) / torch.sqrt(var + epsilon)) + beta)\n",
    "                    clean_bias_data = conv_module.bias.numpy() if hasattr(conv_module.bias, 'numpy') else conv_module.bias\n",
    "                    with open(biases_filename, mode='w', newline='') as file:\n",
    "                        writer = csv.writer(file)\n",
    "                        writer.writerows(clean_bias_data.flatten().reshape(1, -1))\n",
    "            # Reset conv_module after processing BatchNorm folding\n",
    "            conv_module = None\n",
    "\n",
    "        # Handle fully connected layer (fc)\n",
    "        if name == \"fc\":\n",
    "            fc_weights_filename = os.path.join(data_path, f'layer_fc_weight.csv')\n",
    "            fc_bias_filename = os.path.join(data_path, f'layer_fc_bias.csv')\n",
    "            print(f'fc layer weights name: {fc_weights_filename}')\n",
    "            print(f'fc layer bias name: {fc_bias_filename}')\n",
    "            clean_weights_data = module.weight.data.numpy() if hasattr(module.weight.data, 'numpy') else module.weight.data\n",
    "            clean_bias_data = module.bias.data.numpy() if hasattr(module.bias.data, 'numpy') else module.bias.data\n",
    "            with open(fc_weights_filename, mode='w', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerows(clean_weights_data.flatten().reshape(1, -1))\n",
    "            \n",
    "            with open(fc_bias_filename, mode='w', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerows(clean_bias_data.flatten().reshape(1, -1))\n",
    "\n",
    "    print(\"BatchNorm folded into Conv layers and weights/biases saved to CSV.\")\n",
    "    return replace_bn_with_identity(model)\n",
    "\n",
    "def replace_bn_with_identity(model):\n",
    "    \"\"\"\n",
    "    Replaces all BatchNorm layers in the model with Identity layers.\n",
    "    This is done after folding BatchNorm into Conv layers.\n",
    "    \"\"\"\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, nn.BatchNorm2d) or isinstance(module, nn.BatchNorm1d) or isinstance(module, nn.BatchNorm3d):\n",
    "            # Replace with Identity layer\n",
    "            setattr(model, name, nn.Identity())\n",
    "        else:\n",
    "            # Recursively handle nested submodules\n",
    "            replace_bn_with_identity(module)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1.1213,  -6.7830,   6.1559,   4.1597,   3.2178,  -8.5106,   8.5623,\n",
      "         -10.8796,  -5.8799,  -4.5208]], grad_fn=<AddmmBackward0>)\n",
      "weight name: ./../../weights/resnet20/layer0_conv1_weight.csv\n",
      "bias name: ./../../weights/resnet20/layer0_conv1_bias.csv\n",
      "weight name: ./../../weights/resnet20/layer1_block1_conv1_weight.csv\n",
      "bias name: ./../../weights/resnet20/layer1_block1_conv1_bias.csv\n",
      "weight name: ./../../weights/resnet20/layer1_block1_conv2_weight.csv\n",
      "bias name: ./../../weights/resnet20/layer1_block1_conv2_bias.csv\n",
      "weight name: ./../../weights/resnet20/layer1_block2_conv1_weight.csv\n",
      "bias name: ./../../weights/resnet20/layer1_block2_conv1_bias.csv\n",
      "weight name: ./../../weights/resnet20/layer1_block2_conv2_weight.csv\n",
      "bias name: ./../../weights/resnet20/layer1_block2_conv2_bias.csv\n",
      "weight name: ./../../weights/resnet20/layer1_block3_conv1_weight.csv\n",
      "bias name: ./../../weights/resnet20/layer1_block3_conv1_bias.csv\n",
      "weight name: ./../../weights/resnet20/layer1_block3_conv2_weight.csv\n",
      "bias name: ./../../weights/resnet20/layer1_block3_conv2_bias.csv\n",
      "weight name: ./../../weights/resnet20/layer2_block1_conv1_weight.csv\n",
      "bias name: ./../../weights/resnet20/layer2_block1_conv1_bias.csv\n",
      "weight name: ./../../weights/resnet20/layer2_block1_conv2_weight.csv\n",
      "bias name: ./../../weights/resnet20/layer2_block1_conv2_bias.csv\n",
      "weight name: ./../../weights/resnet20/layer2_block1_shortcut_weight.csv\n",
      "bias name: ./../../weights/resnet20/layer2_block1_shortcut_bias.csv\n",
      "weight name: ./../../weights/resnet20/layer2_block2_conv1_weight.csv\n",
      "bias name: ./../../weights/resnet20/layer2_block2_conv1_bias.csv\n",
      "weight name: ./../../weights/resnet20/layer2_block2_conv2_weight.csv\n",
      "bias name: ./../../weights/resnet20/layer2_block2_conv2_bias.csv\n",
      "weight name: ./../../weights/resnet20/layer2_block3_conv1_weight.csv\n",
      "bias name: ./../../weights/resnet20/layer2_block3_conv1_bias.csv\n",
      "weight name: ./../../weights/resnet20/layer2_block3_conv2_weight.csv\n",
      "bias name: ./../../weights/resnet20/layer2_block3_conv2_bias.csv\n",
      "weight name: ./../../weights/resnet20/layer3_block1_conv1_weight.csv\n",
      "bias name: ./../../weights/resnet20/layer3_block1_conv1_bias.csv\n",
      "weight name: ./../../weights/resnet20/layer3_block1_conv2_weight.csv\n",
      "bias name: ./../../weights/resnet20/layer3_block1_conv2_bias.csv\n",
      "weight name: ./../../weights/resnet20/layer3_block1_shortcut_weight.csv\n",
      "bias name: ./../../weights/resnet20/layer3_block1_shortcut_bias.csv\n",
      "weight name: ./../../weights/resnet20/layer3_block2_conv1_weight.csv\n",
      "bias name: ./../../weights/resnet20/layer3_block2_conv1_bias.csv\n",
      "weight name: ./../../weights/resnet20/layer3_block2_conv2_weight.csv\n",
      "bias name: ./../../weights/resnet20/layer3_block2_conv2_bias.csv\n",
      "weight name: ./../../weights/resnet20/layer3_block3_conv1_weight.csv\n",
      "bias name: ./../../weights/resnet20/layer3_block3_conv1_bias.csv\n",
      "weight name: ./../../weights/resnet20/layer3_block3_conv2_weight.csv\n",
      "bias name: ./../../weights/resnet20/layer3_block3_conv2_bias.csv\n",
      "fc layer weights name: ./../../weights/resnet20/layer_fc_weight.csv\n",
      "fc layer bias name: ./../../weights/resnet20/layer_fc_bias.csv\n",
      "BatchNorm folded into Conv layers and weights/biases saved to CSV.\n",
      "tensor([[  1.1213,  -6.7830,   6.1559,   4.1597,   3.2178,  -8.5106,   8.5623,\n",
      "         -10.8796,  -5.8799,  -4.5208]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_364569/1172442577.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('./../models/resnet20_2025-01-23.pth'), strict=False)\n"
     ]
    }
   ],
   "source": [
    "# WEIGHTS_DIR = './../models/resnet20_2025-01-23.pth'\n",
    "data_path = './../../weights/resnet20/'\n",
    "\n",
    "# load weights\n",
    "channel_values = [16, 32, 64]\n",
    "num_classes = 10\n",
    "fake_data = torch.rand((1, 3, 32, 32))\n",
    "model = ResNet20(channel_values, num_classes)\n",
    "\n",
    "model.load_state_dict(torch.load('./../models/resnet20_2025-01-26.pth'), strict=False)\n",
    "model.eval()\n",
    "print(model(fake_data))\n",
    "model = fold_batch_norm(model, data_path)\n",
    "print(model(fake_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize index\n",
    "# i = 0\n",
    "# data_path =\"./../../weights/vgg11/\"\n",
    "\n",
    "# # Iterate over each layer in weights\n",
    "# for layer in weights.keys():\n",
    "#     # Check if the layer is a bias\n",
    "#     data = weights[layer].numpy() if hasattr(weights[layer], 'numpy') else weights[layer]\n",
    "#     if 'bias' in layer:\n",
    "#         # Define CSV file name for biases\n",
    "#         filename = f'{data_path}{layers[i]}_bias.csv'\n",
    "#         # Open the file for writing\n",
    "#         with open(filename, mode='w', newline='') as file:\n",
    "#             writer = csv.writer(file)\n",
    "#             # Write the shape of the bias\n",
    "#             # writer.writerow([f'{layers[i]} bias shape:', data.shape])\n",
    "#             # Write the bias values\n",
    "#             # writer.writerow([f'{layers[i]} bias:'])\n",
    "#             writer.writerows(data.flatten().reshape(1, -1))  # Flatten and reshape to write as single row\n",
    "#         i += 1\n",
    "#     else:\n",
    "#         # Define CSV file name for weights\n",
    "#         filename = f'{data_path}{layers[i]}_weights.csv'\n",
    "#         # Open the file for writing\n",
    "#         with open(filename, mode='w', newline='') as file:\n",
    "#             writer = csv.writer(file)\n",
    "#             # Write the shape of the weights\n",
    "#             # writer.writerow([f'{layers[i]} weights shape:', data.shape])\n",
    "#             # Write the weights values\n",
    "#             # writer.writerow([f'{layers[i]} weights:'])\n",
    "#             writer.writerows(data.flatten().reshape(1, -1))  # Flatten and reshape to write as single row\n",
    "\n",
    "# print(\"All Layers Exported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
