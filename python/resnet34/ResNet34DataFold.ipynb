{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Shape of x_train: (40000, 32, 32, 3)\n",
      "Shape of x_test: (10000, 32, 32, 3)\n",
      "Shape of y_train: (40000,)\n",
      "Shape of y_test: (10000,)\n",
      "[16, 32, 64, 128]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nnjungle/Documents/ppml/secureconnet/python/resnet34/resnet34.py:349: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(save_path), strict=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Accuracy: 73.34%\n",
      "Batch 1\n",
      "Wrong Prediction for Image 1 - True Label: 49, Predicted Label: 90\n",
      "Image 2 - True Label: 33, Predicted Label: 33\n",
      "Image 3 - True Label: 72, Predicted Label: 72\n",
      "Image 4 - True Label: 51, Predicted Label: 51\n",
      "Image 5 - True Label: 71, Predicted Label: 71\n",
      "Wrong Prediction for Image 6 - True Label: 92, Predicted Label: 7\n",
      "Wrong Prediction for Image 7 - True Label: 15, Predicted Label: 8\n",
      "Wrong Prediction for Image 8 - True Label: 14, Predicted Label: 7\n",
      "Wrong Prediction for Image 9 - True Label: 23, Predicted Label: 71\n",
      "Image 10 - True Label: 0, Predicted Label: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import os\n",
    "import csv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import pandas as pd\n",
    "from resnet34 import ResNet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fold_batch_norm(model:nn.Module, data_path):\n",
    "    \"\"\"\n",
    "    Fold BatchNorm layers into Conv layers, then save the weights and biases\n",
    "    of each layer into separate CSV files in the specified `data_path`.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)  # Create the directory if it doesn't exist\n",
    "\n",
    "    conv_module = None\n",
    "    bn_next = False\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for index, (name, module) in enumerate(model.named_modules()):\n",
    "        # Identify BatchNorm layers\n",
    "        # print(f'name:{name}, index: {index}')\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            bn_next = True\n",
    "            conv_module = module\n",
    "        if isinstance(module, nn.BatchNorm2d) and bn_next:\n",
    "            bn_next = False\n",
    "\n",
    "            gamma = module.weight.data  # The scale factor\n",
    "            beta = module.bias.data  # The shift factor\n",
    "            mean = module.running_mean  # The moving average of means\n",
    "            var = module.running_var  # The moving average of variances\n",
    "            epsilon = module.eps  # Epsilon to avoid division by zero\n",
    "\n",
    "            # Compute the new Conv2d weights (fold BatchNorm into Conv)\n",
    "            with torch.no_grad():\n",
    "                # Scale Conv weights by gamma / sqrt(var + epsilon)\n",
    "                conv_module.weight.data = conv_module.weight.data * (gamma / torch.sqrt(var + epsilon)).view(-1, 1, 1, 1)\n",
    "\n",
    "                if \".bn\" in name:\n",
    "                    name_to_save = name.replace(\".bn\", \"_conv\")\n",
    "                elif \".shortcut_bn\" in name:\n",
    "                    name_to_save = name.replace(\".shortcut_bn\", \"_shortcut\")\n",
    "                elif \"bn1\" in name:\n",
    "                    name_to_save = name.replace(\"bn1\", \"layer0_conv1\")\n",
    "\n",
    "                weights_filename = os.path.join(data_path, f'{name_to_save}_weight.csv')\n",
    "                print(f'weight name:{weights_filename}')\n",
    "                clean_data = conv_module.weight.data.numpy() if hasattr(conv_module.weight.data, 'numpy') else conv_module.weight.data\n",
    "                with open(weights_filename, mode='w', newline='') as file:\n",
    "                    writer = csv.writer(file)\n",
    "                    writer.writerows(clean_data.flatten().reshape(1, -1))\n",
    "\n",
    "                # Adjust Conv bias using BatchNorm parameters\n",
    "                if conv_module.bias is not None:\n",
    "                    conv_module.bias.data = (gamma * (conv_module.bias.data - mean) / torch.sqrt(var + epsilon)) + beta\n",
    "                    \n",
    "                    biases_filename = os.path.join(data_path, f'{name_to_save}_bias.csv')\n",
    "                    print(f'bias name:{biases_filename}')\n",
    "                    # pd.DataFrame(conv_module.bias.data).to_csv(biases_filename, index=False)\n",
    "                else:\n",
    "                    # If Conv2d doesn't have bias, add one using BatchNorm parameters\n",
    "                    biases_filename = os.path.join(data_path, f'{name_to_save}_bias.csv')\n",
    "                    print(f'bias name:{biases_filename}')\n",
    "                    conv_module.bias = nn.Parameter((gamma * (-mean) / torch.sqrt(var + epsilon)) + beta)\n",
    "                    clean_bias_data = conv_module.bias.numpy() if hasattr(conv_module.bias, 'numpy') else conv_module.bias\n",
    "                    with open(biases_filename, mode='w', newline='') as file:\n",
    "                        writer = csv.writer(file)\n",
    "                        writer.writerows(clean_bias_data.flatten().reshape(1, -1))\n",
    "                    # pd.DataFrame(conv_module.bias.data).to_csv(biases_filename, index=False)\n",
    "\n",
    "            conv_module = None\n",
    "            # Save the updated weights an\n",
    "        if name == \"fc\":\n",
    "            fc_weights_filename = os.path.join(data_path, f'layer_fc_weight.csv')\n",
    "            fc_bias_filename = os.path.join(data_path, f'layer_fc_bias.csv')\n",
    "            print(f'fc layer weights name:{fc_weights_filename}')\n",
    "            print(f'fc layer bias name:{fc_bias_filename}')\n",
    "            clean_weights_data = module.weight.data.numpy() if hasattr(module.weight.data, 'numpy') else module.weight.data\n",
    "            clean_bias_data = module.bias.data.numpy() if hasattr(module.bias.data, 'numpy') else module.bias.data\n",
    "            with open(fc_weights_filename, mode='w', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerows(clean_weights_data.flatten().reshape(1, -1))\n",
    "            \n",
    "            with open(fc_bias_filename, mode='w', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerows(clean_bias_data.flatten().reshape(1, -1))\n",
    "\n",
    "    print(\"BatchNorm folded into Conv layers and weights/biases saved to CSV.\")\n",
    "    return replace_bn_with_identity(model)\n",
    "\n",
    "def replace_bn_with_identity(model):\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, nn.BatchNorm2d) or isinstance(module, nn.BatchNorm1d) or isinstance(module, nn.BatchNorm3d):\n",
    "            # Replace with Identity layer\n",
    "            setattr(model, name, nn.Identity())\n",
    "        else:\n",
    "            # Recursively handle nested submodules\n",
    "            replace_bn_with_identity(module)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ResNet20' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m      7\u001b[0m fake_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m))\n\u001b[0;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mResNet20\u001b[49m(channel_values, num_classes)\n\u001b[1;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./../models/resnet34_2025-01-19.pth\u001b[39m\u001b[38;5;124m'\u001b[39m), strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ResNet20' is not defined"
     ]
    }
   ],
   "source": [
    "WEIGHTS_DIR = './../models/resnet34_2025-01-19.pth'\n",
    "data_path = './../../weights/resnet34/'\n",
    "\n",
    "# load weights\n",
    "channel_values = [16, 32, 64, 128]\n",
    "print(channel_values)\n",
    "num_classes = 100\n",
    "fake_data = torch.rand((1, 3, 32, 32))\n",
    "model = ResNet34(channel_values, num_classes)\n",
    "\n",
    "model.load_state_dict(torch.load('./../models/resnet34_2025-01-19.pth'), strict=False)\n",
    "model.eval()\n",
    "print(model(fake_data))\n",
    "model = fold_batch_norm(model, data_path)\n",
    "print(model(fake_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize index\n",
    "# i = 0\n",
    "# data_path =\"./../../weights/vgg11/\"\n",
    "\n",
    "# # Iterate over each layer in weights\n",
    "# for layer in weights.keys():\n",
    "#     # Check if the layer is a bias\n",
    "#     data = weights[layer].numpy() if hasattr(weights[layer], 'numpy') else weights[layer]\n",
    "#     if 'bias' in layer:\n",
    "#         # Define CSV file name for biases\n",
    "#         filename = f'{data_path}{layers[i]}_bias.csv'\n",
    "#         # Open the file for writing\n",
    "#         with open(filename, mode='w', newline='') as file:\n",
    "#             writer = csv.writer(file)\n",
    "#             # Write the shape of the bias\n",
    "#             # writer.writerow([f'{layers[i]} bias shape:', data.shape])\n",
    "#             # Write the bias values\n",
    "#             # writer.writerow([f'{layers[i]} bias:'])\n",
    "#             writer.writerows(data.flatten().reshape(1, -1))  # Flatten and reshape to write as single row\n",
    "#         i += 1\n",
    "#     else:\n",
    "#         # Define CSV file name for weights\n",
    "#         filename = f'{data_path}{layers[i]}_weights.csv'\n",
    "#         # Open the file for writing\n",
    "#         with open(filename, mode='w', newline='') as file:\n",
    "#             writer = csv.writer(file)\n",
    "#             # Write the shape of the weights\n",
    "#             # writer.writerow([f'{layers[i]} weights shape:', data.shape])\n",
    "#             # Write the weights values\n",
    "#             # writer.writerow([f'{layers[i]} weights:'])\n",
    "#             writer.writerows(data.flatten().reshape(1, -1))  # Flatten and reshape to write as single row\n",
    "\n",
    "# print(\"All Layers Exported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
